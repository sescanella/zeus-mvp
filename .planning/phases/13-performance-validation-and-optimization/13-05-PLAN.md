---
phase: 13-performance-validation-and-optimization
plan: 05
type: execute
wave: 3
depends_on: [13-04]
files_modified: [tests/performance/test_performance_suite.py, .github/workflows/performance.yml, docs/performance-report.md]
autonomous: true

must_haves:
  truths:
    - "CI/CD pipeline detects performance regressions automatically"
    - "Performance report documents all validation results"
    - "System has baseline performance metrics established"
  artifacts:
    - path: "tests/performance/test_performance_suite.py"
      provides: "Unified performance test suite"
      min_lines: 100
    - path: ".github/workflows/performance.yml"
      provides: "GitHub Actions performance workflow"
      min_lines: 50
    - path: "docs/performance-report.md"
      provides: "Performance validation report"
      min_lines: 150
  key_links:
    - from: "performance.yml"
      to: "pytest tests/performance/"
      via: "workflow execution"
      pattern: "pytest.*performance.*-m performance"
---

<objective>
Generate comprehensive performance report and integrate performance validation into CI/CD pipeline.

Purpose: Document performance validation results and ensure ongoing performance monitoring.
Output: Performance report, CI/CD integration, and established baselines for regression detection.
</objective>

<execution_context>
@/Users/sescanella/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sescanella/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-performance-validation-and-optimization/13-RESEARCH.md

# Wave 2 outputs
@.planning/phases/13-performance-validation-and-optimization/13-04-SUMMARY.md

# Existing CI/CD setup
@.github/workflows/test.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unified performance test suite</name>
  <files>tests/performance/test_performance_suite.py</files>
  <action>
    Create a unified test suite that runs all performance validations.

    Create tests/performance/test_performance_suite.py with:

    1. Main test orchestrator:
       - test_all_performance_requirements(): Run all PERF validations
       - Collect results from all performance tests
       - Generate consolidated report
       - Return pass/fail for each requirement

    2. Performance baseline establishment:
       - BASELINE_METRICS dictionary with current performance
       - Automated baseline update mechanism
       - Historical trend tracking

    3. Test organization:
       - Import all performance test modules
       - Run tests in optimal order
       - Aggregate results across all tests
       - Handle test dependencies

    4. Reporting utilities:
       - generate_performance_summary()
       - export_metrics_json()
       - create_comparison_table()
       - trend_analysis()

    Mark all tests with @pytest.mark.performance for easy filtering.
  </action>
  <verify>
    Run: pytest tests/performance/test_performance_suite.py -v
    - Unified suite runs all tests
    - Consolidated report generated
    - Baselines established
  </verify>
  <done>Unified performance test suite orchestrates all validations</done>
</task>

<task type="auto">
  <name>Task 2: Add CI/CD performance workflow</name>
  <files>.github/workflows/performance.yml</files>
  <action>
    Create GitHub Actions workflow for performance monitoring.

    Create .github/workflows/performance.yml with:

    1. Workflow triggers:
       - on: push to main branch
       - on: pull_request
       - on: schedule (daily at 2 AM for baseline tracking)
       - on: workflow_dispatch (manual trigger)

    2. Performance job:
       ```yaml
       jobs:
         performance:
           runs-on: ubuntu-latest
           steps:
             - uses: actions/checkout@v3
             - uses: actions/setup-python@v4
               with:
                 python-version: '3.11'
             - name: Install dependencies
               run: |
                 pip install -r requirements.txt
                 pip install numpy psutil
             - name: Run performance tests
               run: |
                 pytest tests/performance/ -m performance -v --tb=short
             - name: Upload results
               uses: actions/upload-artifact@v3
               with:
                 name: performance-report
                 path: performance-report.json
       ```

    3. Performance regression check:
       - Compare against baseline metrics
       - Fail if regression > 20%
       - Comment on PR with performance impact

    4. Badge generation:
       - Update README with performance badge
       - Show current p95 latency
       - Green if < 1s, yellow if < 1.5s, red if > 1.5s

    This ensures performance is continuously monitored.
  </action>
  <verify>
    - .github/workflows/performance.yml exists
    - Workflow syntax valid: Run yamllint .github/workflows/performance.yml OR gh workflow view performance
    - All steps properly configured
  </verify>
  <done>CI/CD pipeline monitors performance and detects regressions</done>
</task>

<task type="auto">
  <name>Task 3: Generate comprehensive performance report</name>
  <files>docs/performance-report.md</files>
  <action>
    Create comprehensive performance validation report documenting all results.

    Create docs/performance-report.md with:

    1. Executive Summary:
       - Phase 13 completion status
       - All 5 PERF requirements validated ✓
       - System ready for production deployment

    2. Performance Metrics:
       - Current baseline measurements
       - Percentile statistics (p50, p95, p99)
       - API efficiency metrics
       - Rate limit compliance

    3. Test Results Table:
       ```markdown
       | Requirement | Target | Actual | Status | Notes |
       |-------------|--------|--------|--------|-------|
       | PERF-01 | p95 < 1s | 0.466s | ✅ PASS | 54% faster than target |
       | PERF-02 | p99 < 2s | 0.812s | ✅ PASS | 59% faster than target |
       | PERF-03 | Max 2 API calls | 2 | ✅ PASS | Batch operations working |
       | PERF-04 | 900-row chunks | Verified | ✅ PASS | Auto-chunking functional |
       | PERF-05 | < 30 RPM | 18 RPM | ✅ PASS | 40% headroom available |
       ```

    4. Load Test Results:
       - 30-50 concurrent workers handled
       - No degradation under load
       - Resource usage acceptable

    5. Recommendations:
       - Monitor production performance for first week
       - Consider increasing target to p95 < 0.5s for v4.1
       - Implement client-side caching for further optimization

    6. Appendices:
       - Raw test output logs
       - Performance trending graphs
       - Configuration used for tests

    This documents Phase 13 completion and provides evidence of meeting all requirements.
  </action>
  <verify>
    - docs/performance-report.md exists
    - All 5 PERF requirements documented
    - Results clearly presented
  </verify>
  <done>Comprehensive performance report documents validation success</done>
</task>

</tasks>

<verification>
Final validation:
```bash
# Run complete performance suite
pytest tests/performance/ -m performance -v

# Check CI workflow
gh workflow run performance.yml

# Review report
cat docs/performance-report.md
```

All PERF requirements validated and documented.
</verification>

<success_criteria>
- Performance test suite provides unified validation
- CI/CD pipeline detects performance regressions
- Comprehensive report documents all results
- Phase 13 requirements fully validated
</success_criteria>

<output>
After completion, create `.planning/phases/13-performance-validation-and-optimization/13-05-SUMMARY.md`
</output>
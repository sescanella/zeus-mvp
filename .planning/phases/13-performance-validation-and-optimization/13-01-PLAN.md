---
phase: 13-performance-validation-and-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [tests/performance/test_batch_latency.py, tests/performance/conftest.py, requirements.txt]
autonomous: true

must_haves:
  truths:
    - "System measures p95 latency < 1s for 10-union batch operations"
    - "System measures p99 latency < 2s for 10-union batch operations"
    - "System accurately measures p95 < 1s and p99 < 2s with statistical confidence"
  artifacts:
    - path: "tests/performance/test_batch_latency.py"
      provides: "Percentile-based performance validation"
      min_lines: 150
    - path: "tests/performance/conftest.py"
      provides: "Shared fixtures and utilities"
      min_lines: 50
  key_links:
    - from: "test_batch_latency.py"
      to: "numpy.percentile"
      via: "import and usage"
      pattern: "np\\.percentile\\(latencies, (95|99)\\)"
---

<objective>
Implement percentile-based latency validation for PERF-01 and PERF-02 requirements.

Purpose: Validate that batch union operations meet p95 < 1s and p99 < 2s SLA targets.
Output: Performance tests with numpy percentile calculation and comprehensive reporting.
</objective>

<execution_context>
@/Users/sescanella/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sescanella/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-performance-validation-and-optimization/13-RESEARCH.md

# Existing performance test infrastructure from Phase 8
@tests/performance/test_batch_performance.py
@tests/integration/test_performance_target.py
@backend/services/union_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install numpy and create performance test utilities</name>
  <files>requirements.txt, tests/performance/conftest.py</files>
  <action>
    Install numpy for percentile calculation and create shared performance utilities in conftest.py.

    First, activate the virtual environment and install numpy:
    - Activate venv: source venv/bin/activate
    - Install numpy: pip install numpy
    - Update requirements.txt: pip freeze > requirements.txt

    Then create tests/performance/conftest.py with:
    - calculate_performance_percentiles() function using numpy.percentile()
    - Mock latency calibration fixtures (300ms for batch_update, 150ms for append_rows)
    - Performance reporting utilities for p50/p95/p99 output
    - Time-based measurement helpers

    The percentile calculation must handle edge cases (empty arrays, NaN values) and return a comprehensive stats dictionary with n, avg, min, max, p50, p95, p99.
  </action>
  <verify>
    - numpy appears in requirements.txt
    - tests/performance/conftest.py exists with calculate_performance_percentiles function
    - Function handles empty arrays gracefully
  </verify>
  <done>numpy installed and performance utilities created in conftest.py</done>
</task>

<task type="auto">
  <name>Task 2: Implement p95/p99 latency validation tests</name>
  <files>tests/performance/test_batch_latency.py</files>
  <action>
    Create comprehensive latency validation tests for PERF-01 and PERF-02.

    Create tests/performance/test_batch_latency.py with:

    1. test_10_union_batch_percentiles():
       - Import calculate_performance_percentiles from conftest.py
       - Run 100 iterations of 10-union batch operations
       - Collect latency measurements using time.time()
       - Calculate p50, p95, p99 using numpy.percentile()
       - Assert p95 < 1.0s (PERF-01)
       - Assert p99 < 2.0s (PERF-02)
       - Print comprehensive statistics report

    2. test_cold_vs_warm_cache_performance():
       - Measure cold cache performance (invalidate before each iteration)
       - Measure warm cache performance (reuse cache)
       - Report both scenarios for realistic expectations

    3. test_large_batch_50_unions():
       - Stress test with 50 unions (5x normal load)
       - Verify linear scaling (5x unions should be < 5x latency)
       - Check memory efficiency (< 50MB increase)

    Use mock latency simulation from Phase 8 (300ms for batch_update) but add realistic variance using random.lognormvariate() to model real-world API behavior.
  </action>
  <verify>
    Run: pytest tests/performance/test_batch_latency.py -v
    - All tests pass
    - p95 and p99 statistics are printed
    - Tests use numpy.percentile() for calculations
  </verify>
  <done>Percentile-based latency tests validate PERF-01 (p95 < 1s) and PERF-02 (p99 < 2s)</done>
</task>

<task type="auto">
  <name>Task 3: Add performance regression detection</name>
  <files>tests/performance/test_batch_latency.py</files>
  <action>
    Enhance tests with baseline comparison and regression detection.

    Add to test_batch_latency.py:

    1. PERFORMANCE_BASELINES dictionary:
       - Document current performance (0.466s average from Phase 8)
       - Set regression thresholds (20% degradation triggers failure)

    2. test_performance_no_regression():
       - Compare current performance against baseline
       - Fail if performance degrades > 20% from baseline
       - Log performance trends for monitoring

    3. pytest markers for CI/CD:
       - @pytest.mark.performance for all performance tests
       - @pytest.mark.slow for tests taking > 30 seconds
       - Allow skipping in CI with -m "not slow"

    This ensures performance doesn't degrade over time and provides early warning of issues.
  </action>
  <verify>
    - PERFORMANCE_BASELINES defined in test file
    - Regression test compares against baseline
    - Tests have appropriate pytest markers
  </verify>
  <done>Performance regression detection added to catch degradation early</done>
</task>

</tasks>

<verification>
Run full performance test suite:
```bash
pytest tests/performance/test_batch_latency.py -v -s
```

Expected output shows:
- p95 latency < 1.0s ✓
- p99 latency < 2.0s ✓
- Performance statistics printed
- No regression from baseline
</verification>

<success_criteria>
- numpy installed and available for percentile calculations
- Comprehensive latency tests validate PERF-01 and PERF-02
- Performance regression detection prevents degradation
- Clear reporting of p50, p95, p99 metrics
</success_criteria>

<output>
After completion, create `.planning/phases/13-performance-validation-and-optimization/13-01-SUMMARY.md`
</output>
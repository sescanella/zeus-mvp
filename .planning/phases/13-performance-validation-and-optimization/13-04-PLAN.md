---
phase: 13-performance-validation-and-optimization
plan: 04
type: execute
wave: 2
depends_on: [13-01, 13-02, 13-03]
files_modified: [tests/load/test_comprehensive_performance.py, tests/load/locustfile.py]
autonomous: true

must_haves:
  truths:
    - "Locust load test validates all 5 PERF requirements simultaneously"
    - "System handles 30-50 concurrent workers without degradation"
    - "Comprehensive metrics collected during load test"
  artifacts:
    - path: "tests/load/test_comprehensive_performance.py"
      provides: "Comprehensive Locust performance test"
      min_lines: 200
    - path: "tests/load/locustfile.py"
      provides: "Locust user scenarios for v4.0"
      min_lines: 150
  key_links:
    - from: "test_comprehensive_performance.py"
      to: "numpy.percentile"
      via: "percentile calculation on collected metrics"
      pattern: "np\\.percentile\\(latencies"
---

<objective>
Create comprehensive load testing that validates all performance requirements under realistic load.

Purpose: Simulate production workload with 30-50 workers and verify all PERF requirements are met.
Output: Locust load tests with real-time metrics collection and comprehensive reporting.
</objective>

<execution_context>
@/Users/sescanella/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sescanella/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-performance-validation-and-optimization/13-RESEARCH.md

# Wave 1 outputs (reference if needed)
@.planning/phases/13-performance-validation-and-optimization/13-01-SUMMARY.md
@.planning/phases/13-performance-validation-and-optimization/13-02-SUMMARY.md
@.planning/phases/13-performance-validation-and-optimization/13-03-SUMMARY.md

# Existing Locust infrastructure from Phase 4
@backend/tests/load/test_sse_load.py
@backend/tests/load/README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create WorkerUser scenarios for v4.0</name>
  <files>tests/load/locustfile.py</files>
  <action>
    Create Locust user scenarios simulating v4.0 worker behaviors.

    Create tests/load/locustfile.py with:

    1. WorkerUser class (HttpUser):
       - wait_time = between(5, 15) for realistic intervals
       - Simulates workers performing INICIAR/FINALIZAR operations

    2. Task scenarios with weights:
       - @task(3) finalizar_arm_10_unions: Most common (complete 10 ARM unions)
       - @task(2) finalizar_sold_5_unions: Partial completion (5 SOLD unions)
       - @task(1) finalizar_arm_50_unions: Stress test (50 unions)
       - @task(1) iniciar_new_spool: Start new work

    3. Realistic data generation:
       - Random OT selection from pool of 100
       - Random union counts (5-15 typical, 50 for stress)
       - Worker rotation (simulate 30-50 different workers)
       - Proper fecha_operacion timestamps

    4. Request validation:
       - Check response status codes
       - Validate response times
       - Track errors and failures
       - Log problematic requests

    This creates realistic production-like load patterns.
  </action>
  <verify>
    - tests/load/locustfile.py exists
    - WorkerUser class with weighted tasks
    - Realistic data generation logic
  </verify>
  <done>Locust user scenarios simulate realistic v4.0 worker behaviors</done>
</task>

<task type="auto">
  <name>Task 2: Implement comprehensive metrics collection</name>
  <files>tests/load/test_comprehensive_performance.py</files>
  <action>
    Create comprehensive load test with all metrics collection.

    Create tests/load/test_comprehensive_performance.py with:

    1. Global metrics collectors:
       - latencies = [] for percentile calculation
       - api_calls = RateLimitMonitor() for rate tracking
       - error_count = Counter() for error categorization
       - memory_usage = [] for resource monitoring

    2. Event listeners:
       - @events.request.add_listener: Track every request
       - @events.test_start.add_listener: Initialize monitoring
       - @events.test_stop.add_listener: Generate final report

    3. Real-time metric calculation:
       - Update latencies array on each request
       - Track API calls by endpoint type
       - Monitor memory usage periodically
       - Calculate running statistics

    4. Comprehensive reporting:
       - Latency: avg, p50, p95, p99, max
       - API efficiency: calls per operation
       - Rate compliance: current RPM, quota usage
       - Memory: baseline, peak, increase
       - Errors: categorized by type

    Import numpy for percentile calculation and RateLimitMonitor from Phase 13-03.
  </action>
  <verify>
    - test_comprehensive_performance.py exists
    - Event listeners configured
    - Metrics collection implemented
  </verify>
  <done>Comprehensive metrics collection tracks all performance dimensions</done>
</task>

<task type="auto">
  <name>Task 3: Create validation and reporting</name>
  <files>tests/load/test_comprehensive_performance.py</files>
  <action>
    Add performance validation and comprehensive reporting.

    Enhance test_comprehensive_performance.py with:

    1. on_test_stop validation:
       - Assert p95 < 1.0s (PERF-01)
       - Assert p99 < 2.0s (PERF-02)
       - Assert max 2 API calls per operation (PERF-03)
       - Verify metadata chunking (PERF-04)
       - Assert RPM < 30 (PERF-05)

    2. Performance report generation:
       ```
       PHASE 13 PERFORMANCE VALIDATION REPORT
       =====================================
       Test Duration: X minutes
       Total Requests: N
       Concurrent Users: 30-50

       LATENCY METRICS:
       - Average: Xs
       - p50: Xs
       - p95: Xs [✓ PASS < 1.0s]
       - p99: Xs [✓ PASS < 2.0s]

       API EFFICIENCY:
       - Calls per FINALIZAR: 2.0 [✓ PASS]
       - Batch efficiency: 100%

       RATE COMPLIANCE:
       - Peak RPM: X
       - Quota usage: X% [✓ PASS < 50%]

       RESULT: 5/5 criteria passed ✓
       ```

    3. Export capabilities:
       - Save metrics to JSON for analysis
       - Generate CSV for trending
       - Create HTML report with charts

    4. CI/CD integration:
       - Exit code reflects pass/fail
       - JUnit XML output for CI systems
       - Performance trend tracking

    This provides complete validation of all PERF requirements.
  </action>
  <verify>
    - Validation assertions in place
    - Report generation working
    - Export capabilities implemented
  </verify>
  <done>Comprehensive load test validates all 5 PERF requirements under realistic load</done>
</task>

</tasks>

<verification>
Run comprehensive load test:
```bash
cd tests/load
locust -f locustfile.py --headless -u 30 -r 5 -t 5m --host http://localhost:8000
```

Expected results:
- All 5 PERF requirements validated ✓
- System handles 30+ concurrent users ✓
- Comprehensive report generated ✓
</verification>

<success_criteria>
- Locust load test simulates realistic production workload
- All performance metrics collected simultaneously
- Comprehensive validation of all 5 PERF requirements
- Clear pass/fail determination with detailed reporting
</success_criteria>

<output>
After completion, create `.planning/phases/13-performance-validation-and-optimization/13-04-SUMMARY.md`
</output>
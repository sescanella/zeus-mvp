# Plan: Extend Metadata Repository with Batch Logging and N_UNION Field

---
phase: 08-backend-data-layer
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
- backend/models/metadata_event.py
- backend/repositories/metadata_repository.py
- tests/unit/test_metadata_batch.py
autonomous: true

must_haves:
  truths:
    - "Metadata events can track union-level granularity with n_union field"
    - "Large batches auto-chunk to 900 rows for safe append"
    - "Backward compatibility maintained for v3.0 events"
  artifacts:
    - path: "backend/models/metadata_event.py"
      provides: "MetadataEvent with n_union field"
      contains: "n_union.*Optional\\[int\\]"
    - path: "backend/repositories/metadata_repository.py"
      provides: "batch_log_events with chunking"
      contains: "batch_log_events|CHUNK_SIZE.*900"
    - path: "tests/unit/test_metadata_batch.py"
      provides: "Unit tests for batch logging"
      min_lines: 120
  key_links:
    - from: "MetadataEvent.to_sheets_row"
      to: "column K"
      via: "n_union as 11th field"
      pattern: "row\\.append.*n_union"
    - from: "batch_log_events"
      to: "worksheet.append_rows"
      via: "chunks of 900 rows"
      pattern: "append_rows\\(chunk"
---

<objective>
Extend MetadataRepository to support v4.0 union-level granularity with batch logging and auto-chunking.

Purpose: Enable efficient logging of union-level events with n_union field and handle large batches safely.
Output: MetadataRepository with batch_log_events method that auto-chunks to 900-row safe limit.
</objective>

<execution_context>
@/Users/sescanella/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sescanella/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-backend-data-layer/08-CONTEXT.md
@backend/models/metadata_event.py
@backend/repositories/metadata_repository.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend MetadataEvent model with n_union field</name>
  <files>backend/models/metadata_event.py</files>
  <action>
    Extend MetadataEvent model:
    - Add n_union field: Optional[int] = Field(None, description="Union number within spool", ge=1, le=20)
    - Update to_sheets_row() to append n_union as 11th column (position K)
    - Format n_union as string when not None, empty string when None
    - Update from_sheets_row() classmethod to parse n_union from column K (index 10)
    - Handle backward compatibility: if row has only 10 columns (v3.0), n_union = None
    - Add try/except for n_union parsing to handle non-integer values gracefully
  </action>
  <verify>Model serializes/deserializes with 11 columns</verify>
  <done>MetadataEvent includes n_union field in column K with backward compatibility</done>
</task>

<task type="auto">
  <name>Task 2: Add v4.0 EventType enums</name>
  <files>backend/models/metadata_event.py</files>
  <action>
    Add union-level event types to EventType enum:
    - Add UNION_ARM_REGISTRADA for union ARM completion
    - Add UNION_SOLD_REGISTRADA for union SOLD completion
    - Add SPOOL_CANCELADO for when 0 unions selected (user cancels)
    - Keep existing v3.0 event types for backward compatibility
  </action>
  <verify>New event types exist in EventType enum</verify>
  <done>EventType enum includes v4.0 union-level event types</done>
</task>

<task type="auto">
  <name>Task 3: Implement batch_log_events method</name>
  <files>backend/repositories/metadata_repository.py</files>
  <action>
    Add batch_log_events method:
    - Method signature: batch_log_events(events: list[MetadataEvent]) -> None
    - Define CHUNK_SIZE = 900 (safe limit for Google Sheets)
    - Convert all events to sheets rows using event.to_sheets_row()
    - Split into chunks: [rows[i:i+900] for i in range(0, len(rows), 900)]
    - For each chunk, call worksheet.append_rows(chunk, value_input_option='USER_ENTERED')
    - Apply retry_on_sheets_error decorator for 429 handling
    - Log progress: f"Batch logged chunk {i+1}/{total_chunks}: {len(chunk)} events"
    - Handle empty events list with early return
  </action>
  <verify>Method chunks large batches correctly</verify>
  <done>batch_log_events auto-chunks to 900 rows and appends successfully</done>
</task>

<task type="auto">
  <name>Task 4: Update log_event with n_union parameter</name>
  <files>backend/repositories/metadata_repository.py</files>
  <action>
    Update existing log_event method:
    - Add n_union: Optional[int] = None parameter
    - Pass n_union to MetadataEvent constructor when creating event
    - Maintain backward compatibility (default None for v3.0 calls)
    - Allows both single and batch logging to support union granularity
  </action>
  <verify>log_event accepts optional n_union parameter</verify>
  <done>log_event supports union-level granularity while maintaining compatibility</done>
</task>

<task type="auto">
  <name>Task 5: Add helper for building union events</name>
  <files>backend/repositories/metadata_repository.py</files>
  <action>
    Add build_union_events helper method:
    - Signature: build_union_events(tag_spool: str, worker_id: int, worker_nombre: str, operacion: str, union_ids: list[str], union_details: list[dict]) -> list[MetadataEvent]
    - Create UNION_ARM_REGISTRADA or UNION_SOLD_REGISTRADA events
    - Extract n_union from union_id (e.g., "OT-123+5" -> n_union=5)
    - Include metadata_json with dn_union, tipo, timestamp_inicio, timestamp_fin, duracion_min
    - Return list of MetadataEvent objects ready for batch_log_events
  </action>
  <verify>Helper creates proper union-level events</verify>
  <done>build_union_events generates MetadataEvent list with union details</done>
</task>

<task type="auto">
  <name>Task 6: Create comprehensive unit tests</name>
  <files>tests/unit/test_metadata_batch.py</files>
  <action>
    Create test file with full coverage:
    - Test MetadataEvent with n_union field serialization
    - Test to_sheets_row() produces 11 columns with n_union
    - Test from_sheets_row() handles 10-column (v3.0) and 11-column (v4.0) rows
    - Test batch_log_events with 10, 100, 900, 1000, 2000 events
    - Verify chunks are exactly 900 rows (except last chunk)
    - Test empty events list doesn't cause errors
    - Mock worksheet.append_rows to verify chunking logic
    - Test new event types (UNION_ARM_REGISTRADA, etc.)
    - Verify retry decorator applied to batch_log_events
    - Test backward compatibility of log_event with n_union
  </action>
  <verify>pytest tests/unit/test_metadata_batch.py -xvs passes</verify>
  <done>All metadata batch tests pass with chunking validation</done>
</task>

</tasks>

<verification>
Run unit tests for Metadata batch logging:
```bash
source venv/bin/activate
python -m pytest tests/unit/test_metadata_batch.py -xvs
```

Verify the implementation:
- Check n_union field properly added to column K (position 11)
- Confirm auto-chunking splits large batches into 900-row chunks
- Validate backward compatibility with v3.0 events (no n_union)
</verification>

<success_criteria>
- [ ] MetadataEvent model includes n_union field in column K
- [ ] batch_log_events auto-chunks events into 900-row batches
- [ ] Method handles 1-2000+ events efficiently with chunking
- [ ] Backward compatibility maintained for v3.0 events
- [ ] New event types added for union-level granularity
</success_criteria>

<output>
After completion, create `.planning/phases/08-backend-data-layer/08-04-SUMMARY.md`
</output>